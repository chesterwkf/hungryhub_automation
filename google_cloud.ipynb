{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication to Google API\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "from google.cloud import vision\n",
    "import re\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='/Users/Cwkf_89/Library/CloudStorage/OneDrive-SingaporeManagementUniversity/Y3S2/data analytics/Project/google_vision_key.json'\n",
    "WORD = re.compile(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    # for non-dense text \n",
    "    # response = client.text_detection(image=image)\n",
    "    # for dense text\n",
    "    response = client.document_text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    ocr_text = []\n",
    "\n",
    "    for text in texts:\n",
    "        ocr_text.append(f\"\\r\\n{text.description}\")\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            \"{}\\nFor more info on error messages, check: \"\n",
    "            \"https://cloud.google.com/apis/design/errors\".format(response.error.message)\n",
    "        )\n",
    "    return ocr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./Menu_8.png\"\n",
    "text = detect_text(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\nBRUNCH\\nsalmon\\ncured\\npaina\\n&\\ncream\\n340.-\\nlay low breakfast\\nhouse cured salmon | dill cream\\npickled shallot caper | sliced radish\\ndill oil salad\\ntomato & burrata\\n280.-\\negg & avocado\\n260.\\nscrambled eggs | baked beans | bacon\\ngrilled mushroom & onion\\nhouse-baked sourdough\\nshrimp on toast\\n300.-\\n300.-\\nmarinated tomatoes | burrata cheese\\nparsley oil | house-baked sourdough\\ngrilled cheese toast\\n{260.-\\nsmashed avocado | poached eggs\\nsalad | house-baked sourdough\\ncroque madame\\nshrimp mayonnaise | herb yogurt\\ndill oil shrimp oil pickled shallot\\nhouse-baked sourdough & salad\\n300.-\\ncheddar, mozzarella, parmesan & gruyere cheese\\nhouse-baked sourdough\\ntomato sauce & salad\\nhÃ¡m cheddar | mozzarella | parmesan\\ngruyere cheese mornay sauce\\nhouse-baked sourdough\\nsunny side up egg I tomato sauce & salad\\n++ add on ++\\n+sous vide egg\\n+sourdough\\n+avocado\\n+burrata\\n+cured salmon\\n30-\\n30-\\n80.-\\n100-\\n100.-\\nN\\now'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-67e679ba-0b5fd33103c690c10ae27691;03ab8e8b-ea57-479f-9402-dff07cb4053c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:862\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:969\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:1486\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1482\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1483\u001b[39m ):\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1488\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:1376\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:1296\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:280\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/file_download.py:304\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    303\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:426\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 401 Client Error. (Request ID: Root=1-67e679ba-0b5fd33103c690c10ae27691;03ab8e8b-ea57-479f-9402-dff07cb4053c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# Example OCR text\u001b[39;00m\n\u001b[32m    136\u001b[39m     ocr_text = text[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[43mprocess_menu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mprocess_menu\u001b[39m\u001b[34m(ocr_text)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_menu\u001b[39m(ocr_text):\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     parser = \u001b[43mMenuParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     items_prices = parser.extract_items_and_prices(ocr_text)\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mMenuParser.__init__\u001b[39m\u001b[34m(self, model_name)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the menu parser with a specified open-source LLM.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    model_name: The Hugging Face model to use for extraction\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load model and tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.model = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     15\u001b[39m     model_name, \n\u001b[32m     16\u001b[39m     torch_dtype=torch.bfloat16,  \u001b[38;5;66;03m# Use lower precision for efficiency\u001b[39;00m\n\u001b[32m     17\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Automatically use available hardware\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:930\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m         config = AutoConfig.for_model(**config_dict)\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m config_tokenizer_class = config.tokenizer_class\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1096\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1093\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1094\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1097\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1098\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/configuration_utils.py:594\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    593\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    596\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/configuration_utils.py:653\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    649\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    652\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gcp-cloud-vision/lib/python3.12/site-packages/transformers/utils/hub.py:481\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    484\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-67e679ba-0b5fd33103c690c10ae27691;03ab8e8b-ea57-479f-9402-dff07cb4053c)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "class MenuParser:\n",
    "    def __init__(self, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
    "        \"\"\"Initialize the menu parser with a specified open-source LLM.\n",
    "        \n",
    "        Args:\n",
    "            model_name: The Hugging Face model to use for extraction\n",
    "        \"\"\"\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            torch_dtype=torch.bfloat16,  # Use lower precision for efficiency\n",
    "            device_map=\"auto\"  # Automatically use available hardware\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and normalize OCR text.\"\"\"\n",
    "        # Normalize line breaks\n",
    "        text = re.sub(r'\\r\\n|\\r', '\\n', text)\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        # Merge lines that likely belong together\n",
    "        lines = text.split('\\n')\n",
    "        merged_lines = []\n",
    "        buffer = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # If the line contains a price pattern, it's likely the end of an item\n",
    "            if re.search(r'\\d+\\.-', line):\n",
    "                if buffer:\n",
    "                    merged_lines.append(buffer)\n",
    "                merged_lines.append(line)\n",
    "                buffer = \"\"\n",
    "            else:\n",
    "                # Add space if buffer is not empty\n",
    "                if buffer:\n",
    "                    buffer += \" \" + line\n",
    "                else:\n",
    "                    buffer = line\n",
    "        \n",
    "        # Don't forget remaining buffer\n",
    "        if buffer:\n",
    "            merged_lines.append(buffer)\n",
    "            \n",
    "        return \"\\n\".join(merged_lines)\n",
    "    \n",
    "    def extract_items_and_prices(self, text):\n",
    "        \"\"\"Use LLM to extract menu items and prices.\"\"\"\n",
    "        cleaned_text = self.preprocess_text(text)\n",
    "        \n",
    "        # Create prompt for the LLM\n",
    "        prompt = f\"\"\"<s>[INST] \n",
    "        You are a specialized assistant for restaurant menu analysis.\n",
    "        \n",
    "        Extract all menu items and their prices from the following OCR-extracted menu text.\n",
    "        Format your response as a JSON array of objects with 'item' and 'price' fields.\n",
    "        For items with descriptions, include the main item name only.\n",
    "        Only include items that have clear prices.\n",
    "        \n",
    "        Here's the OCR text:\n",
    "        \n",
    "        {cleaned_text}\n",
    "        [/INST]\"\"\"\n",
    "        \n",
    "        # Generate response from the model\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate with appropriate parameters for a structured response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=2048,\n",
    "                temperature=0.1,  # Low temperature for more deterministic output\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the model's response (after the prompt)\n",
    "        response = response.split(\"[/INST]\")[-1].strip()\n",
    "        \n",
    "        # Use regex to isolate the JSON part\n",
    "        json_match = re.search(r'\\[\\s*{.*}\\s*\\]', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            import json\n",
    "            try:\n",
    "                items_prices = json.loads(json_match.group(0))\n",
    "                return items_prices\n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON parsing fails, return a simple extraction based on regex\n",
    "                return self._fallback_extraction(cleaned_text)\n",
    "        else:\n",
    "            return self._fallback_extraction(cleaned_text)\n",
    "    \n",
    "    def _fallback_extraction(self, text):\n",
    "        \"\"\"Fallback method using regex if LLM parsing fails.\"\"\"\n",
    "        items_prices = []\n",
    "        # Basic regex to find potential item-price pairs\n",
    "        pattern = r'([a-zA-Z\\s&]+)(?:\\n|.)*?(\\d+\\.-)'\n",
    "        matches = re.finditer(pattern, text)\n",
    "        \n",
    "        for match in matches:\n",
    "            item = match.group(1).strip()\n",
    "            price = match.group(2).strip()\n",
    "            if item and price and not item.isdigit():\n",
    "                items_prices.append({\"item\": item, \"price\": price})\n",
    "        \n",
    "        return items_prices\n",
    "\n",
    "# Example usage\n",
    "def process_menu(ocr_text):\n",
    "    parser = MenuParser()\n",
    "    items_prices = parser.extract_items_and_prices(ocr_text)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Found {len(items_prices)} menu items:\")\n",
    "    for item in items_prices:\n",
    "        print(f\"{item['item']} - {item['price']}\")\n",
    "    \n",
    "    return items_prices\n",
    "\n",
    "# If run directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Example OCR text\n",
    "    ocr_text = text[0]\n",
    "    \n",
    "    process_menu(ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "image=Image.open(image_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing with huggingface api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def extract_menu_with_huggingface(ocr_text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract menu items using Hugging Face Inference API (free tier).\"\"\"\n",
    "    \n",
    "    # Hugging Face API endpoint (free tier, no authentication needed for some models)\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    \n",
    "    # Create structured prompt for the model\n",
    "    prompt = f\"\"\"<s>[INST] I have OCR text from a restaurant menu. Please extract each menu item and its price, \n",
    "    and return a JSON array of objects with 'item' and 'price' fields.\n",
    "    \n",
    "    Please ignore category headers like \"BEVERAGES\", \"coffee\", \"non-coffee\", etc. Also ignore restaurant names.\n",
    "    Format prices as integers (e.g., \"85.-\" should be 85).\n",
    "    \n",
    "    Here's the OCR text:\n",
    "    {ocr_text}\n",
    "    \n",
    "    Return only a valid JSON array like:\n",
    "    [\n",
    "      {{\"item\": \"americano\", \"price\": 85}},\n",
    "      {{\"item\": \"caffe latte\", \"price\": 100}}\n",
    "    ]\n",
    "    [/INST]</s>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make request to Hugging Face\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 1024}}\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=data)\n",
    "    result = response.json()\n",
    "    \n",
    "    # Extract JSON part from response\n",
    "    if isinstance(result, list) and \"generated_text\" in result[0]:\n",
    "        response_text = result[0][\"generated_text\"]\n",
    "        # Find the JSON part in the response using regex\n",
    "        json_match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                menu_items = json.loads(json_match.group(0))\n",
    "                return menu_items\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error parsing JSON from response\")\n",
    "                return []\n",
    "    \n",
    "    print(\"Unexpected response format from API\")\n",
    "    return []\n",
    "\n",
    "# Example usage\n",
    "def process_menu(ocr_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Process menu OCR text into a structured DataFrame.\"\"\"\n",
    "    # Get menu items from Hugging Face\n",
    "    menu_items = extract_menu_with_huggingface(ocr_text)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if menu_items:\n",
    "        return pd.DataFrame(menu_items)\n",
    "    else:\n",
    "        # Fallback to basic pattern matching if API fails\n",
    "        return basic_pattern_extraction(ocr_text)\n",
    "\n",
    "def basic_pattern_extraction(ocr_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Fallback method using regex pattern matching.\"\"\"\n",
    "    lines = ocr_text.strip().split('\\n')\n",
    "    menu_items = []\n",
    "    \n",
    "    # Simple pattern matching for item-price pairs\n",
    "    for i in range(len(lines) - 1):\n",
    "        current_line = lines[i].strip()\n",
    "        next_line = lines[i + 1].strip()\n",
    "        \n",
    "        # Skip category headers and empty lines\n",
    "        if not current_line or current_line.upper() == current_line:\n",
    "            continue\n",
    "            \n",
    "        # Check if next line contains a price pattern\n",
    "        price_match = re.search(r'(\\d+)\\.?-+', next_line)\n",
    "        if price_match:\n",
    "            price = int(price_match.group(1))\n",
    "            # Skip if the item name is very short or looks like a category\n",
    "            if len(current_line) > 2 and not current_line.isupper():\n",
    "                menu_items.append({\n",
    "                    \"item\": current_line,\n",
    "                    \"price\": price\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(menu_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing with Ollama api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def process_with_ollama(ocr_text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process menu text using Ollama local LLM.\n",
    "    \n",
    "    Prerequisites:\n",
    "    1. Install Ollama from https://ollama.ai/\n",
    "    2. Run: ollama pull mistral or ollama pull llama2\n",
    "    3. Start Ollama server\n",
    "    \"\"\"\n",
    "    # Ollama API endpoint (running locally)\n",
    "    API_URL = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Prompt with clear instructions\n",
    "    prompt = f\"\"\"\n",
    "    Task: Extract menu items and prices from this OCR text of a restaurant menu.\n",
    "    \n",
    "    OCR Text:\n",
    "    {ocr_text}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Identify each menu item and its corresponding price\n",
    "    2. Ignore category headers (like \"BEVERAGES\", \"coffee\", etc.)\n",
    "    3. Ignore restaurant names or repeated text at the bottom\n",
    "    4. Format each price as an integer (remove the \".-\" suffix)\n",
    "    \n",
    "    Return ONLY a valid JSON array with this structure:\n",
    "    [\n",
    "      {{\"item\": \"item name\", \"price\": price_as_integer}},\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure Ollama request\n",
    "    payload = {\n",
    "        \"model\": \"llama2\",  # or \"llama2\" if you prefer\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if \"response\" in result:\n",
    "            # Extract JSON array from the response\n",
    "            response_text = result[\"response\"]\n",
    "            json_match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', response_text, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                try:\n",
    "                    menu_items = json.loads(json_match.group(0))\n",
    "                    return menu_items\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error parsing JSON from Ollama response\")\n",
    "                    return []\n",
    "        \n",
    "        print(\"Unexpected response format from Ollama\")\n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        print(\"Make sure Ollama is installed and running locally\")\n",
    "        return []\n",
    "\n",
    "def categorize_items(menu_items: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"Categorize menu items based on their names.\"\"\"\n",
    "    df = pd.DataFrame(menu_items)\n",
    "    \n",
    "    # Add category column\n",
    "    df['category'] = 'OTHER'\n",
    "    \n",
    "    # Define category patterns\n",
    "    categories = {\n",
    "        'COFFEE': ['americano', 'espresso', 'latte', 'cappuccino', 'mocha', 'macchiato', 'flat white', 'cortado', 'affogato'],\n",
    "        'TEA': ['tea'],\n",
    "        'NON-COFFEE': ['chocolate', 'frappe', 'soda', 'lemonade', 'passion fruit', 'peach']\n",
    "    }\n",
    "    \n",
    "    # Assign categories based on item names\n",
    "    for item_idx, item_name in enumerate(df['item']):\n",
    "        item_lower = item_name.lower()\n",
    "        for cat, keywords in categories.items():\n",
    "            if any(keyword in item_lower for keyword in keywords):\n",
    "                df.at[item_idx, 'category'] = cat\n",
    "                break\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_menu(ocr_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Process menu OCR text into a structured DataFrame.\"\"\"\n",
    "    # Try Ollama first\n",
    "    try:\n",
    "        menu_items = process_with_ollama(ocr_text)\n",
    "        if menu_items:\n",
    "            return categorize_items(menu_items)\n",
    "    except:\n",
    "        print(\"Ollama processing failed, falling back to pattern matching\")\n",
    "    \n",
    "    # Fallback to pattern matching\n",
    "    return basic_pattern_extraction(ocr_text)\n",
    "\n",
    "def basic_pattern_extraction(ocr_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Fallback method using regex pattern matching.\"\"\"\n",
    "    lines = ocr_text.strip().split('\\n')\n",
    "    menu_items = []\n",
    "    \n",
    "    # Simple pattern matching for item-price pairs\n",
    "    for i in range(len(lines) - 1):\n",
    "        current_line = lines[i].strip()\n",
    "        next_line = lines[i + 1].strip()\n",
    "        \n",
    "        # Skip category headers and empty lines\n",
    "        if not current_line or current_line.upper() == current_line:\n",
    "            continue\n",
    "            \n",
    "        # Check if next line contains a price pattern\n",
    "        price_match = re.search(r'(\\d+)\\.?-+', next_line)\n",
    "        if price_match:\n",
    "            price = int(price_match.group(1))\n",
    "            # Skip if the item name is very short or looks like a category\n",
    "            if len(current_line) > 2 and not current_line.isupper():\n",
    "                menu_items.append({\n",
    "                    \"item\": current_line,\n",
    "                    \"price\": price\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and categorize\n",
    "    df = pd.DataFrame(menu_items)\n",
    "    if not df.empty:\n",
    "        return categorize_items(df.to_dict('records'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected response format from API\n",
      "                            item  price\n",
      "0                          cream    340\n",
      "1               tomato & burrata    280\n",
      "2                shrimp on toast    300\n",
      "3           grilled cheese toast    260\n",
      "4  house-baked sourdough & salad    300\n",
      "5                  +cured salmon     30\n"
     ]
    }
   ],
   "source": [
    "# To use in your notebook:\n",
    "\n",
    "ocr_text = text[0]  # Get the full text from first element\n",
    "\n",
    "# Process the menu\n",
    "menu_df = process_menu(ocr_text)\n",
    "print(menu_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def clean_text(ocr_text: str) -> str:\n",
    "    \"\"\"Clean the OCR text.\"\"\"\n",
    "    # Fix common OCR issues\n",
    "    text = ocr_text.replace('--', '-')\n",
    "    text = re.sub(r'\\r', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_menu_items(ocr_text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract menu items using rule-based pattern matching.\"\"\"\n",
    "    # Clean the text\n",
    "    clean_ocr = clean_text(ocr_text)\n",
    "    \n",
    "    # Split into lines\n",
    "    lines = clean_ocr.strip().split('\\n')\n",
    "    \n",
    "    # Define words to skip (category headers, etc.)\n",
    "    skip_words = {'beverages', 'coffee', 'non-coffee', 'organic tea', 'add-on'}\n",
    "    \n",
    "    # Extract menu items\n",
    "    menu_items = []\n",
    "    current_category = \"UNCATEGORIZED\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Check if this is a category header\n",
    "        if line.isupper() and len(line) > 2:\n",
    "            current_category = line\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Skip known headers\n",
    "        if line.lower() in skip_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Look ahead for price pattern\n",
    "        if i + 1 < len(lines):\n",
    "            next_line = lines[i + 1].strip()\n",
    "            price_match = re.search(r'(\\d+)\\.?-+', next_line)\n",
    "            \n",
    "            if price_match:\n",
    "                # Found a price, extract the item and price\n",
    "                price = int(price_match.group(1))\n",
    "                \n",
    "                # Check if this is a valid menu item (not too short, not all caps)\n",
    "                if len(line) > 2 and not line.isupper():\n",
    "                    menu_items.append({\n",
    "                        \"category\": current_category,\n",
    "                        \"item\": line,\n",
    "                        \"price\": price\n",
    "                    })\n",
    "                \n",
    "                # Skip the price line\n",
    "                i += 2\n",
    "                continue\n",
    "        \n",
    "        # If we didn't find a price pattern, move to the next line\n",
    "        i += 1\n",
    "    \n",
    "    return menu_items\n",
    "\n",
    "def categorize_items(menu_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Categorize menu items based on item names.\"\"\"\n",
    "    # Define category keywords\n",
    "    category_patterns = {\n",
    "        \"COFFEE\": ['coffee', 'americano', 'espresso', 'latte', 'cappuccino', 'mocha', 'macchiato', 'flat white', 'cortado', 'affogato'],\n",
    "        \"TEA\": ['tea'],\n",
    "        \"NON-COFFEE\": ['chocolate', 'frappe', 'soda', 'lemonade']\n",
    "    }\n",
    "    \n",
    "    # Process each item\n",
    "    for item in menu_items:\n",
    "        # Skip if already has a good category\n",
    "        if item[\"category\"] != \"UNCATEGORIZED\":\n",
    "            continue\n",
    "            \n",
    "        item_name = item[\"item\"].lower()\n",
    "        \n",
    "        # Check each category pattern\n",
    "        for category, patterns in category_patterns.items():\n",
    "            if any(pattern in item_name for pattern in patterns):\n",
    "                item[\"category\"] = category\n",
    "                break\n",
    "    \n",
    "    return menu_items\n",
    "\n",
    "def identify_add_ons(menu_items: List[Dict[str, Any]], ocr_text: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"Identify add-on items.\"\"\"\n",
    "    # Look for the add-on section\n",
    "    add_on_match = re.search(r'add-on(.*?)(?:\\n\\w+:|$)', ocr_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    regular_items = []\n",
    "    add_ons = []\n",
    "    \n",
    "    if add_on_match:\n",
    "        add_on_section = add_on_match.group(1)\n",
    "        \n",
    "        # Extract add-on items\n",
    "        add_on_items = re.findall(r'-\\s*(.*?)\\s*\\+\\s*(\\d+)\\.?-+', add_on_section)\n",
    "        \n",
    "        for name, price in add_on_items:\n",
    "            add_ons.append({\n",
    "                \"category\": \"ADD-ON\",\n",
    "                \"item\": name.strip(),\n",
    "                \"price\": int(price)\n",
    "            })\n",
    "    \n",
    "    # Separate regular items from potential add-ons\n",
    "    for item in menu_items:\n",
    "        # Check if the item is already found in add-ons\n",
    "        if any(add_on[\"item\"] == item[\"item\"] for add_on in add_ons):\n",
    "            continue\n",
    "            \n",
    "        # Check if item has '+' in the name or price\n",
    "        if \"+\" in item[\"item\"] or (isinstance(item[\"price\"], str) and \"+\" in item[\"price\"]):\n",
    "            # This is likely an add-on\n",
    "            clean_item = item[\"item\"].replace(\"+\", \"\").strip()\n",
    "            clean_price = str(item[\"price\"]).replace(\"+\", \"\").strip()\n",
    "            \n",
    "            try:\n",
    "                price = int(clean_price)\n",
    "                add_ons.append({\n",
    "                    \"category\": \"ADD-ON\",\n",
    "                    \"item\": clean_item,\n",
    "                    \"price\": price\n",
    "                })\n",
    "            except ValueError:\n",
    "                # If we can't parse the price, just add it to regular items\n",
    "                regular_items.append(item)\n",
    "        else:\n",
    "            regular_items.append(item)\n",
    "    \n",
    "    return regular_items, add_ons\n",
    "\n",
    "def process_menu(ocr_text: str) -> pd.DataFrame:\n",
    "    \"\"\"Process the OCR text into a structured menu DataFrame.\"\"\"\n",
    "    # Extract initial menu items\n",
    "    menu_items = extract_menu_items(ocr_text)\n",
    "    \n",
    "    # Categorize items\n",
    "    categorized_items = categorize_items(menu_items)\n",
    "    \n",
    "    # Identify add-ons\n",
    "    regular_items, add_ons = identify_add_ons(categorized_items, ocr_text)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(regular_items + add_ons)\n",
    "    \n",
    "    # Sort by category and item\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=[\"category\", \"item\"])\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category                      item  price\n",
      "26     ADD-ON         - extra shot 25.-     50\n",
      "27     ADD-ON           - oat milk 25.-    120\n",
      "23     ADD-ON                extra shot     25\n",
      "25     ADD-ON                  oat milk     25\n",
      "24     ADD-ON             special beans     50\n",
      "17  BEVERAGES               affogato***    140\n",
      "19  BEVERAGES            black rose tea    120\n",
      "1   BEVERAGES               caffe latte    100\n",
      "3   BEVERAGES               caffe mocha    125\n",
      "15  BEVERAGES            cappuccino (H)    110\n",
      "5   BEVERAGES         caramel macchiato    140\n",
      "13  BEVERAGES               cortado (H)    110\n",
      "9   BEVERAGES                     dirty    110\n",
      "21  BEVERAGES             earl grey tea    120\n",
      "20  BEVERAGES     english breakfast tea    120\n",
      "16  BEVERAGES              espresso (H)     70\n",
      "11  BEVERAGES            flat white (H)    110\n",
      "18  BEVERAGES                 green tea    120\n",
      "22  BEVERAGES         herbal blends tea    120\n",
      "14  BEVERAGES             lemonade soda    120\n",
      "12  BEVERAGES  mango passion fruit soda    120\n",
      "2   BEVERAGES              matcha latte    140\n",
      "7   BEVERAGES         orangina espresso    130\n",
      "10  BEVERAGES                peach soda    120\n",
      "8   BEVERAGES     strawberry lemon soda    120\n",
      "6   BEVERAGES           thai tea frappe    140\n",
      "4   BEVERAGES            thai tea latte    120\n",
      "0   BEVERAGES        valrhona chocolate    140\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = detect_text(image_path)\n",
    "ocr_text = text[0]  # Get the full text\n",
    "\n",
    "# Process the menu\n",
    "menu_df = process_menu(ocr_text)\n",
    "print(menu_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[{'generated_text': \"Bonjour, c'est-Ã -dire?\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
    "headers = {\"Authorization\": f\"Bearer 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\"}\n",
    "data = {\"inputs\": \"Translate to French: Hello, how are you?\"}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-cloud-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
